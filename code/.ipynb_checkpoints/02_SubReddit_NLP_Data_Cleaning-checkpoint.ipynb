{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png)  Project 3: Web APIs & Classification\n",
    "Reddit's API:  Data Wrangling, Natural Language Processing, and Classification Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project covers three of the biggest concepts in Data Science:\n",
    "- Data Wrangling/Acquisition\n",
    "- Natural Language Processing\n",
    "- Classification Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Technical Report:   *SubReddit Data Cleaning (NLP I)*\n",
    "This notebook --just one component of the overall project-- reflects the collection, import (and cleaning?? ) of two subreddits of my choosing. . .\n",
    "\n",
    "Part 1 of the project focuses on **Data wrangling/gathering/acquisition**. \n",
    "The expectatiion is that not all acquired data will be clean or in a structured/organized format (like a single .csv file or SQL table). While an API request for data is ideal, some scraping may be required if the website of interest does not have an API (or it's terribly documented).\n",
    "\n",
    ". . . At the end of this notebook, scraped (& cleaned?? ) data is saved to .csv datasets which can be referenced here:\n",
    "- `dataset1.csv`:  [subreddit_Today_I_Learned](../data/dataset1.csv)\n",
    "- `subreddit_Health.csv`:  [subreddit topic: Health](../data/subreddit_Health.csv)\n",
    "\n",
    "Ultimately this data will be used with NLP to train a classifier on which subreddit a given post came from. **This is a binary classification problem**.\n",
    "\n",
    "\n",
    "\n",
    "**Data Cleaning and EDA**\n",
    "- Are missing values imputed/handled appropriately?\n",
    "- Are distributions examined and described?\n",
    "- Are outliers identified and addressed?\n",
    "- Are appropriate summary statistics provided?\n",
    "- Are steps taken during data cleaning and EDA framed appropriately?\n",
    "- Does the student address whether or not they are likely to be able to answer their problem statement with the provided data given what they've discovered during EDA?\n",
    "\n",
    "**Preprocessing and Modeling**\n",
    "- Is text data successfully converted to a matrix representation?\n",
    "- Are methods such as stop words, stemming, and lemmatization explored?\n",
    "- Does the student properly split and/or sample the data for validation/training purposes?\n",
    "- Does the student test and evaluate a variety of models to identify a production algorithm (**AT MINIMUM:** Bayes and one other model)?\n",
    "- Does the student defend their choice of production model relevant to the data at hand and the problem?\n",
    "- Does the student explain how the model works and evaluate its performance successes/downfalls?\n",
    "\n",
    "**Evaluation and Conceptual Understanding**\n",
    "- Does the student accurately identify and explain the baseline score?\n",
    "- Does the student select and use metrics relevant to the problem objective?\n",
    "- Does the student interpret the results of their model for purposes of inference?\n",
    "- Is domain knowledge demonstrated when interpreting results?\n",
    "- Does the student provide appropriate interpretation with regards to descriptive and inferential statistics?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal in modeling:  \"generalize estimates well\"\n",
    "\n",
    "### X / Y relationship\n",
    " - \"x & y relationship is the signal; all else is noise...\"\n",
    " - X = \"predictors\"\n",
    " - y = \"predictions\"  \n",
    " \n",
    "         - regression:  y is continuous\n",
    "         - classification: y is discrete  *(probability that y is one of two binary classes)\n",
    "\n",
    "\n",
    "### Bias / Variance Trade-off\n",
    "- under-fit = high bias\n",
    "- over-fit = high variance (too specific)\n",
    "\n",
    "    variance mitigation:\n",
    "    - more data\n",
    "    - fewer features\n",
    "    - REGULARIZATION  (\"alpha\" = the strength of regularization)\n",
    "        - always scale features before you regularize\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "##### (3) types of model errors:\n",
    " - Bias:  \"how bad our model is at predicting \"True y\" (\"too simple\")...\n",
    "          - bias gets smaller as our PREDICTION gets closer to the true value\n",
    " - Variance:  \"how bad our model is at generalizing to new data\" / \"how spread out our predictions are\"\n",
    "          - variance gets higher, the more variables we have (\"too complex\")...\n",
    " - Irreducable:\n",
    " \n",
    " \n",
    "\n",
    "Goal in model evaluation:  Minimize errors to find the \"line of best fit\" (between features & target)\n",
    "\n",
    "### (4) steps to build a model:\n",
    " - 1. instantiate model\n",
    " - 2. fit model to train data:\n",
    "      - model.fit(X, y)\n",
    "      - get coefficients (to determine bias)\n",
    "      - get intercepts (to determine bias)\n",
    " - 3. generate predictions\n",
    "      - model.predict(X)\n",
    " - 4. evaluate model\n",
    "      - decide which model evaluation metrics (loss functions) to use...\n",
    "          - y.mean = naive baseline prediction\n",
    "          - MSE = loss function\n",
    "          - if MSE is smaller than baseline, it means \"on average\" our residuals (errors) are smaller and we have a better fit...\n",
    "          - if R2 is larger than baseline, we have a better fit...\n",
    "\n",
    "         - regression:  evaluation metrics are for ERRORS\n",
    "         - classification:  evaluation metrics are for ACCURACY\n",
    "             - accuracy\n",
    "             - misclassification\n",
    "             - sensitivity (\"recall\")\n",
    "             - specificity\n",
    "             - precision\n",
    "\n",
    "\n",
    "\n",
    "### Logistic Regression\n",
    " - logit link function:  \"the log of the odds of success\"\n",
    " - the null model = the most frequent class\n",
    " - interpret the coefficient for a given variable by \"exponentiating\" it (np.exp(lr.coef_)\n",
    " - deal with UNBALANCED CLASSES via bias correction, weighting observations, stratified cross validation...\n",
    " \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Natural language processing (NLP) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing (NLP) describes the field of getting computers to understand language how we as humans do. Natural language processing has many, many applications including:\n",
    "- voice-to-text services for people who are hard of hearing.\n",
    "- text-to-voice services for people who have difficulty reading.\n",
    "- automated chatbots for organizations.\n",
    "- translation services.\n",
    "\n",
    "Generally when we get text data, strings aren't broken out into individual words or even sentences. We might have a full tweet, full chapter of a book, or full .pdf file all in one long string.\n",
    "\n",
    "Today, we're diving into the practical side of NLP - taking text data and breaking it out into words that we can then leverage into $n$-grams or $tf$-$idf$ vectorizers.\n",
    "\n",
    "#### Agenda\n",
    "- Pre-Processing\n",
    "- Sentiment Analysis\n",
    "\n",
    "#### Learning Objectives\n",
    "- Define and implement tokenizing, lemmatizing, and stemming.\n",
    "- Describe what RegEx does.\n",
    "- Apply sentiment analysis.\n",
    "- Preprocess text data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP I: Tokenizing/Lemmatization and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries required for Natural Language Processing\n",
    "import nltk\n",
    "\n",
    "# Import Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Import Regular Expressions\n",
    "import regex as re\n",
    "\n",
    "# import Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Import Stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# import BeautifulSoup\n",
    "from bs4 import BeautifulSoup             \n",
    "\n",
    "# Import stopwords.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# Import CountVectorizer.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ------------------------------------\n",
    "# Import logistic regression.\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module required for Regular Expressions\n",
    "# !pip install regex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required to make API requests\n",
    "# import requests\n",
    "# # required to throttle your scraping loop... \n",
    "# import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries used for data analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "When dealing with text data, there are some common pre-processing steps we might use. However, we won't necessarily use all of them every time we deal with text data.\n",
    "- Tokenizing\n",
    "- Regular Expression\n",
    "- Lemmatizing/Stemming\n",
    "- Cleaning (i.e. removing HTML)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing\n",
    "When we \"tokenize\" data, we take it and split it up into distinct chunks based on some pattern. A RegexpTokenizer splits a string into substrings using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+') ## We'll talk about this in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Run\" Tokenizer\n",
    "spam_tokens = tokenizer.tokenize(spam.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Results\n",
    "spam_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expressions\n",
    "Regular Expressions, or RegEx, are an extraordinarily helpful way for us to detect patterns in text.\n",
    "This is a tool of which you should be aware.\n",
    "\n",
    "Using RegEx can be incredibly helpful if you want to find text matching a specific pattern.\n",
    "- People used to use two spaces after a period to split sentences up; you could use RegEx to detect that pattern and tokenize on entire sentences.\n",
    "- Chapters in a book could be titled \"Chapter\" followed by a number; you could use RegEx to detect that pattern and tokenize a book by its chapters.\n",
    "- When Python libraries are upgraded, syntax changes! Perhaps you want to detect a certain pattern of syntax so you can update your code efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in spam_tokens:\n",
    "    print(re.findall('\\d+', i), i)\n",
    "\n",
    "# RegEx in Python 3 understands \\d+ to identify numeric digits. \n",
    "# Therefore, the above code searched through spam_tokens to see if any numeric digits were in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate tokenizer.\n",
    "tokenizer_1 = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "# Run tokenizer.\n",
    "tokenizer_1.tokenize(s)\n",
    "\n",
    "# tokenizer_1 splits tokens up by spaces or by periods that are not attached to a digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate tokenizer.\n",
    "tokenizer_2 = RegexpTokenizer('\\s+', gaps=True)\n",
    "\n",
    "# Run tokenizer.\n",
    "tokenizer_2.tokenize(s)\n",
    "\n",
    "# tokenizer_2 will identify the spaces. By setting gaps = True, we're grabbing everything else: thus, we're splitting our tokens up by spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate tokenizer.\n",
    "tokenizer_3 = RegexpTokenizer('[A-Z]\\w+')\n",
    "\n",
    "# Run tokenizer.\n",
    "tokenizer_3.tokenize(s)\n",
    "\n",
    "# tokenizer_3 returns only words that begin with a capital letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing & Stemming\n",
    "- \"He is running really fast!\"\n",
    "- \"He ran the race.\"\n",
    "- \"He runs a five-minute mile.\"\n",
    "\n",
    "If we wanted a computer to interpret these sentences, I might count up how many instances of each word I observe. The computer will treat words like \"running,\" \"ran,\" and \"runs\" differently although they mean about the same thing (in this context).\n",
    "Lemmatizing and stemming are two forms of shortening words so we can combine similar forms of the same word.\n",
    "\n",
    "When we \"lemmatize\" data, we take words and attempt to return their lemma, or the base/dictionary form of a word.\n",
    "\n",
    "Lemmatizing is usually the more correct and precise way of handling things from a grammatical/morphological point of view, but also might not have much of an effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lemmatizer. \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize tokens.\n",
    "tokens_lem = [lemmatizer.lemmatize(i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tokens to lemmatized version.\n",
    "list(zip(spam_tokens, tokens_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print only those lemmatized tokens that are different.\n",
    "for i in range(len(spam_tokens)):\n",
    "    if spam_tokens[i] != tokens_lem[i]:\n",
    "        print((spam_tokens[i], tokens_lem[i]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also do this on individual words.\n",
    "# Lemmatize the word \"computers.\"\n",
    "lemmatizer.lemmatize(\"computers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we \"stem\" data, we take words and attempt to return a base form of the word. It tends to be cruder than using lemmatization. There's a method developed by Porter in 1980 that explains the algorithm used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate object of class PorterStemmer.\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem tokens.\n",
    "stem_spam = [p_stemmer.stem(i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tokens to stemmed version.\n",
    "list(zip(spam_tokens, stem_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print only those stemmed tokens that are different.\n",
    "\n",
    "for i in range(len(spam_tokens)):\n",
    "    if spam_tokens[i] != stem_spam[i]:\n",
    "        print((spam_tokens[i], stem_spam[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also do this on individual words as well.\n",
    "\n",
    "# Stem the word \"computer.\"\n",
    "p_stemmer.stem(\"computer\")\n",
    "\n",
    "# Stem the word \"computation.\"\n",
    "p_stemmer.stem(\"computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "Let's start with a very simple example.\n",
    "Sentiment analysis is an area of natural language processing in which we seek to classify text as having positive or negative emotion.\n",
    "Let's build a simple function that can classify text as either having positive or negative sentiment.\n",
    "What words tell us whether certain text is positive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's come up with a list of positive and negative words we might observe.\n",
    "\n",
    "positive_words = ['delight', 'good', 'great', 'awesome', 'tremendous', 'fabulous', 'amazing', 'stellar']\n",
    "negative_words = ['garbage', 'sad', 'trash', 'ugly', 'bad', 'disgusting', 'terrible', 'gross']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function\n",
    "def simple_sentiment(text):\n",
    "    # Instantiate tokenizer.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # Tokenize text.\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    # Instantiate stemmer.\n",
    "    p_stemmer = PorterStemmer()\n",
    "    \n",
    "    # Stem words.\n",
    "    stemmed_words = [p_stemmer.stem(i) for i in tokens]\n",
    "    \n",
    "    # Stem our positive/negative words.\n",
    "    positive_stems = [p_stemmer.stem(i) for i in positive_words]\n",
    "    negative_stems = [p_stemmer.stem(i) for i in negative_words]\n",
    "\n",
    "    # Count \"positive\" words.\n",
    "    positive_count = sum([1 for i in stemmed_words if i in positive_stems])\n",
    "    \n",
    "    # Count \"negative\" words\n",
    "    negative_count = sum([1 for i in stemmed_words if i in negative_stems])\n",
    "    \n",
    "    # Calculate Sentiment Percentage \n",
    "    # (Positive Count - Negative Count) / (Total Count)\n",
    "\n",
    "    return round((positive_count - negative_count) / len(tokens), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our sentiment analyzer on our spam email.\n",
    "simple_sentiment(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for given text (ex: yelp review)\n",
    "# Calculate sentiment of yelp_1.\n",
    "simple_sentiment(yelp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Positive from Negative Reviews\n",
    "The easiest way to do sentiment classification of analysis is by training a model on data we've already labeled.\n",
    "\n",
    "This is a huge consideration for your capstone, even if it isn't related to NLP!\n",
    "\n",
    "Today, we will begin by reviewing the basic NLP techniques we learned yesterday to create a sentiment analyzer from Rotten Tomatoes Movie review. This code-along is adapted from Kaggle's tutorial, available here.\n",
    "\n",
    "#### Step One: Import The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas.\n",
    "import pandas as pd       \n",
    "\n",
    "# Read in training data.\n",
    "train = pd.read_csv(\"../labeledTrainData.tsv\",\n",
    "                    header=0,\n",
    "                    delimiter=\"\\t\",\n",
    "                    quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first five rows.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the first review.\n",
    "train['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in testing data.\n",
    "test = pd.read_csv(\"../testData.tsv\",\n",
    "                   header=0, \n",
    "                   delimiter=\"\\t\",\n",
    "                   quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first five rows.\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that our Kaggle data was organized train.csv and test.csv. However, the test.csv didn't contain any labels!\n",
    "\n",
    "Let's do a train/test split by splitting up train.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create train_test_split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[['id','review']],\n",
    "                                                    train['sentiment'],\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few steps we'll take to clean up the text data before it's ready for processing.\n",
    "- Remove the HTML code artifacts from the text.\n",
    "- Remove punctuation.\n",
    "- Remove stopwords. (We'll cover these shortly.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step One: Remove HTML code artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BeautifulSoup object on a single movie review     \n",
    "example1 = BeautifulSoup(X_train['review'][0])\n",
    "\n",
    "# Print the raw review and then the output of get_text(), for \n",
    "# comparison\n",
    "print(X_train['review'][0])\n",
    "print()\n",
    "print(example1.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Two: Remove Punctuation\n",
    "Punctuation can be removed using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regular expressions to do a find-and-replace\n",
    "letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      example1.get_text())   # The text to search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first fifty letters of letters_only.\n",
    "letters_only[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert letters_only to lower case.\n",
    "lower_case = letters_only.lower()\n",
    "\n",
    "# Split lower_case up at each space.\n",
    "words = lower_case.split() # This is like a manual tokenizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first ten words.\n",
    "words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Three: Remove Stop Words\n",
    "With our Yelp reviews above, you noticed that our sentiment scores were right around zero. While there were some positive and negative words, the vast majority of the words had neither a positive sentiment nor negative sentiment!\n",
    "- Examples include \"the,\" \"of,\" \"and,\" \"a,\" \"to,\" and \"in.\"\n",
    "\n",
    "Stopwords are very common words that are often removed because they amount to unnecessary information and removing them can dramatically speed up processing.\n",
    "If you didn't complete the NLTK download, you may run into some issues here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print English stopwords.\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from \"words.\"\n",
    "words = [w for w in words if not w in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check \"words\" to make sure we did this properly.\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Four: Combine our cleaning into one function\n",
    "Check: Why should we do everything with one function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "    # 1. Remove HTML.\n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "    \n",
    "    # 2. Remove non-letters.\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words.\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    # a list, so convert the stop words to a set.\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    # 5. Remove stop words.\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Five (Finally!) Applying our Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of reviews based on the dataframe size.\n",
    "total_reviews = train.shape[0]\n",
    "print(f'There are {total_reviews} reviews.')\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews.\n",
    "clean_train_reviews = []\n",
    "clean_test_reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning and parsing the training set movie reviews...\")\n",
    "\n",
    "j = 0\n",
    "for train_review in X_train['review']:\n",
    "    # Convert review to words, then append to clean_train_reviews.\n",
    "    clean_train_reviews.append(review_to_words(train_review))\n",
    "    \n",
    "    # If the index is divisible by 1000, print a message\n",
    "    if (j + 1) % 1000 == 0:\n",
    "        print(f'Review {j + 1} of {total_reviews}.')\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "# Let's do the same for our testing set.\n",
    "\n",
    "print(\"Cleaning and parsing the testing set movie reviews...\")\n",
    "\n",
    "for test_review in X_test['review']:\n",
    "    # Convert review to words, then append to clean_train_reviews.\n",
    "    clean_test_reviews.append(review_to_words(test_review))\n",
    "    \n",
    "    # If the index is divisible by 1000, print a message\n",
    "    if (j + 1) % 1000 == 0:\n",
    "        print(f'Review {j + 1} of {total_reviews}.')\n",
    "        \n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our data is finally ready....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer...\n",
    "You'll describe this in greater detail in a later lesson, but CountVectorizer will transform the lists of the cleaned reviews above into features that we can pass into a model.\n",
    "It will create columns (also knon as vectors), where each column counts how many times each word is observed in each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = None,\n",
    "                             max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array.\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have an array that we can use for classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import logistic regression.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate logistic regression model.\n",
    "\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model to training data.\n",
    "\n",
    "lr.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on training data.\n",
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on testing data.\n",
    "\n",
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple things to note:\n",
    "\n",
    "NLP broadly describes:\n",
    "- how we can get unstructured text data into a more structured form that can be interpreted by computers and\n",
    "- algorithms for interpreting text data.\n",
    "That does not mean these tools we used today work to the exclusion of other methods. You can and should include other variables in your model!\n",
    "- For example, maybe the length of a review tells us something about how much people liked/disliked the movie, or maybe additional information about the reviewer (i.e. geography, age, how many reviews they had submitted) has predictive value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SubReddit Data Cleaning (NLP I) is complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed to the next notebook:\n",
    "- [SubReddit_NLP_Vectorizing](03_SubReddit_NLP_Vectorizing.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. \"text feature extraction\"..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Do all cleaning / de-duping B4 vectorizing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex Cleaning\n",
    "\n",
    "Let's use regex to remove the words `snake, snakes, spider, spiders`. Let's also remove *any* mention of a subreddit, as well as all URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any URLs\n",
    "# subreddits['title'] = subreddits.title.map(lambda x: re.sub('http[s]?:\\/\\/[^\\s]*', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "# subreddits['title'] = subreddits.title.map(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove specific text\n",
    "# subreddits['title'] = subreddits.title.map(lambda x: re.sub('[Weekly thread]*', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase everything\n",
    "# subreddits.title.lower()\n",
    "\n",
    "# # per lesson 5.03:\n",
    "# # Convert letters_only to lower case.\n",
    "# lower_case = subreddits['title'].lower()\n",
    "# # Split lower_case up at each space.\n",
    "# words = lower_case.split() # This is like a manual tokenizer!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
